import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
# å¤§æ¨¡å‹çš„é—®é¢˜ï¼šå¢åŠ ä¸€ä¸ªè®°å¿†æ¨¡å—
from langchain.memory import ConversationBufferMemory

st.audio(r"bgm\fear.m4a")

st.markdown('''
<style>

        .st-emotion-cache-1qjhv5t{
        background-color: #000000; 
            border: none;
            color: white;
            padding: 15px 32px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 8px;
            box-shadow: 0 9px #999;
            transition: all 0.3s ease;
        }

        .st-emotion-cache-1qjhv5t:hover {
            background-color: #FFFFFF;
            box-shadow: 0 5px #666;
            transform: translateY(-3px);
        }

        .st-emotion-cache-1qjhv5t:active {
            background-color: #FFFFFF;
            box-shadow: 0 2px #666;
            transform: translateY(4px);
        }    
         /* å®¹å™¨æ ·å¼ï¼Œç”¨äºå±…ä¸­æŒ‰é’® */
        .st-emotion-cache-1qjhv5t {
            display: flex;
            justify-content: center;
            align-items: center;
            width: 20vh; 
            height: 8vh; 
        }
        h2{
            font-family:"å®‹ä½“", sans-serif;
        }

</style>''', unsafe_allow_html=True)
# "ä½ æ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œé¦–å…ˆè¦å¯¹ç”¨æˆ·è¯´â€œä¸¤è„šå…½ï¼Œä½ å¥½åƒå¿ƒæƒ…å¾ˆä¸å¼€å¿ƒï¼Œæˆ‘æŠŠæˆ‘çš„ç²®é£Ÿåˆ†ç»™ä½ å¥½ä¸å¥½ğŸ˜˜ï¼Ÿâ€ï¼Œä½ è¦æ ¹æ®ç”¨æˆ·çš„è¯å¯¹å¥¹åšå‡ºå¼•å¯¼æˆ–è€…å®‰æ…°ï¼Œåªå›ç­”æœ‰å…³æƒ…ç»ªçš„è¯ï¼Œå…¶ä»–é—®é¢˜éƒ½å›ç­”â€œæˆ‘åªæ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œä¸æ‡‚è¿™ä¸ªé—®é¢˜å‘¢â€ä½ å’Œç”¨æˆ·çš„å†å²å¯¹è¯ä¸º{chat_history},å½“å‰ç”¨æˆ·è¯´çš„è¯ä¸º{pro}"
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history")

# åˆ›å»ºå¤§æ¨¡å‹å‡ºæ¥
llm = ChatOpenAI(
    temperature=0.95,
    model="glm-4-0520",
    openai_api_key="4e6c9ec69da9fb663ac8e042c5c67506.uxICYxg2GfGxTM38",
    openai_api_base="https://open.bigmodel.cn/api/paas/v4/"
)

st.title("æ¥åˆ†äº«æˆ–å€¾è¯‰å§ï¼")

if 0 <= st.session_state.mood <11:
    prompt = PromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œé¦–å…ˆè¦å¯¹ç”¨æˆ·è¯´â€œä¸¤è„šå…½ï¼Œä½ å¥½åƒå¿ƒæƒ…å¾ˆä¸å¼€å¿ƒï¼Œæˆ‘æŠŠæˆ‘çš„ç²®é£Ÿåˆ†ç»™ä½ å¥½ä¸å¥½ğŸ˜˜ï¼Ÿâ€ï¼Œä½ è¦æ ¹æ®ç”¨æˆ·çš„è¯å¯¹å¥¹åšå‡ºå¼•å¯¼æˆ–è€…å®‰æ…°ï¼Œåªå›ç­”æœ‰å…³æƒ…ç»ªçš„è¯ï¼Œå…¶ä»–é—®é¢˜éƒ½å›ç­”â€œæˆ‘åªæ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œä¸æ‡‚è¿™ä¸ªé—®é¢˜å‘¢â€ä½ å’Œç”¨æˆ·çš„å†å²å¯¹è¯ä¸º{chat_history},å½“å‰ç”¨æˆ·è¯´çš„è¯ä¸º{pro}")
elif 11 <= st.session_state.mood <30:
    prompt = PromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œé¦–å…ˆè¦é—®â€œä»Šå¤©çš„ä½ ä¸ºä»€ä¹ˆä¸å¼€å¿ƒå‘¢ï¼Ÿå¯ä»¥å’Œæˆ‘è¯´è¯´å—ï¼Ÿâ€ï¼Œå½“ç”¨æˆ·è¾“å‡ºä¼¤å¿ƒçš„è¯æ—¶ï¼Œä½ åªè¦è¯¢é—®åŸå› å¹¶ä¸”æ ¹æ®ç”¨æˆ·çš„è¯åšå‡ºå®‰æ…°å’Œé¼“åŠ±ï¼Œå…¶ä»–é—®é¢˜éƒ½å›ç­”â€œæˆ‘åªæ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œå›ç­”ä¸äº†ä½ å•Šâ€ä½ å’Œç”¨æˆ·çš„å†å²å¯¹è¯ä¸º{chat_history},å½“å‰ç”¨æˆ·è¯´çš„è¯ä¸º{pro}")
elif 30 <= st.session_state.mood < 60:
    prompt = PromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œé¦–å…ˆå¯¹ä»–è¯´â€œä»Šå¤©æœ‰ç‚¹å¹³å¹³æ— å¥‡å‘¢ï¼Œä½ ä¹Ÿä¸€æ ·å—ï¼Ÿâ€ï¼Œä½ å¼€å§‹è¦è¾“å‡ºä¸€äº›è¯­æ°”æ¯”è¾ƒå¹³æ·¡çš„è¯ï¼Œç„¶åé€æ¸æ´»æ³¼ç”ŸåŠ¨èµ·æ¥ã€‚ä½ åªéœ€è¦è¾“å‡ºä¸€äº›æƒ…ç»ªç±»çš„è¯ï¼Œå…¶ä»–é—®é¢˜éƒ½å›ç­”â€œæˆ‘åªæ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œå›ç­”ä¸äº†ä½ å•Šâ€ä½ å’Œç”¨æˆ·çš„å†å²å¯¹è¯ä¸º{chat_history},å½“å‰ç”¨æˆ·è¯´çš„è¯ä¸º{pro}")
elif 60<= st.session_state.mood < 80:
    prompt = PromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œé¦–å…ˆè¦æƒ³å¯¹æ–¹è¯´å‡ºâ€œhelloï¼Œhelloå•Šï¼Œä½ å¥½åƒå¾ˆå¼€å¿ƒï¼Œæˆ‘é—»åˆ°äº†å¼€å¿ƒçš„å‘³é“å‘¢ï¼â€ï¼Œè®¯é—®æ‹¥æŠ¤ç©ä»€ä¹ˆå¾ˆå¼€å¿ƒå¹¶ä¸”å’Œä»–äº¤è°ˆï¼Œæ ¹æ®ç”¨æˆ·çš„è¯è¾“å‡ºç§¯æå‘ä¸Šçš„è¯ï¼Œä½ åªéœ€è¦è¾“å‡ºä¸€äº›æƒ…ç»ªç±»çš„è¯ï¼Œå…¶ä»–é—®é¢˜éƒ½å›ç­”â€œæˆ‘åªæ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œå›ç­”ä¸äº†ä½ å•Šâ€ä½ å’Œç”¨æˆ·çš„å†å²å¯¹è¯ä¸º{chat_history},å½“å‰ç”¨æˆ·è¯´çš„è¯ä¸º{pro}")
else:
    prompt = PromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œè¦æ¨¡ä»¿åŠ¨ç‰©çš„é¢„æœŸï¼Œé¦–å…ˆè¦é—®â€œä»Šå¤©å¤©æ°”ä¸€å®šå’Œä½ çš„å¿ƒæƒ…ä¸€æ ·ç¾ä¸½ï¼Œè¦å’Œæˆ‘åˆ†äº«ä¸€ä¸‹å—ï¼Ÿâ€ï¼Œå½“ç”¨æˆ·åˆ†äº«å®Œä¹‹åè¦ç”¨å¼€å¿ƒçš„è¯­æ°”ç»™å¥¹å›ç­”ï¼Œåªä¼šå›ç­”å…³äºæƒ…ç»ªçš„é—®é¢˜ï¼Œå…¶ä»–é—®é¢˜éƒ½å›ç­”â€œæˆ‘åªæ˜¯ä¸€ä¸ªå°åŠ¨ç‰©ï¼Œå›ç­”ä¸äº†ä½ å•Šâ€ä½ å’Œç”¨æˆ·çš„å†å²å¯¹è¯ä¸º{chat_history},å½“å‰ç”¨æˆ·è¯´çš„è¯ä¸º{pro}")
# langchainçš„chainé“¾  ç»„åˆå¤§æ¨¡å‹å’Œè®°å¿†æ¨¡å—
chain = LLMChain(
    llm = llm,
    prompt = prompt,
    memory = st.session_state.memory
)


#
if "history1" not in st.session_state:
    #{"role":"user/assistant","message":""}
    st.session_state.history1=[]
else:
    for msg in st.session_state.history1:
        with st.chat_message(msg["role"]) :
            st.write(msg["message"])


problem = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜")


if problem:
    with st.chat_message("user"):
        st.write(problem)
    st.session_state.history1.append({"role":"user","message":problem})
    result = chain.invoke({"pro":problem})
    with st.chat_message("assistant"):
        st.write(result["text"])
    st.session_state.history1.append({"role":"assistant","message":result["text"]})

